---
title: "Getting Data from the Web: SEC's EDGAR"
author: "Matthias Uckert"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

------------------------------------------------------------------------

# Description

In this session we will retrieve information from the SEC EDGAR database. Getting data from EDGAR is exemplary for getting information from the web, where we have API (Application Programming Interface) access.

Throughout this script we will use custom functions to download and transform data. All the script specific functions are stored in this folder: **1_code/00_functions/f-get_edgar_data.R**


EDGAR is an established source of financial information. But normally companies and researchers start gathering financial information from commercial databases such as **Bureau Van Dijk - Orbis**, **Refinitiv - Datastream** or **Compustat**

Compustat: https://wrds-www.wharton.upenn.edu/
Orbis: https://orbis4.bvdinfo.com/ip

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_knit$set(root.dir = normalizePath(here::here())) 
knitr::opts_chunk$set(root.dir = normalizePath(here::here())) 
```

# Script Setup

```{r message=FALSE, warning=FALSE}
library(tidyverse); library(edgarWebR); library(lubridate); library(here)
library(furrr); library(stringi); library(textstem); library(tidytext)
library(janitor); library(tools); library(patchwork); library(scales)
library(kableExtra); library(openxlsx)

source("1_code/00_functions/f-all.R")
source("1_code/00_functions/f-get_edgar_data.R")
```

# Code

## Paths

```{r paths}
lst_paths <- list(
  dir_main    = "2_output/01_get_edgar_data/",
  path_filing = "2_output/01_get_edgar_data/edgar_filings.rds",
  path_detail = "2_output/01_get_edgar_data/edgar_details.rds",
  path_f500   = "2_output/01_get_edgar_data/f500.xlsx"
) %>% create_dirs()

```

## Get Fortune 500 Companies

Her we use an open source data repository 'datahub.io' to retrieve the [**fortune 500 companies**]{.ul} with some financial information. We won't use the complete data set, but filter for the top 100 companies (using less companies will let the code run faster) with the highest market capitalization. You can change the number of companies by changing the integer vector: **.n_companies** in the chunk below.

We use a custom function: [get_f500]{.ul}**()** to download the Fortune 500 dataset.

### Function: [get_f500()]{.ul}

```{r echo=FALSE}
print(get_f500)
```

### Download Dataset

```{r get-fortune500-firms, message=FALSE, warning=FALSE}
.n_companies <- 100
  # browseURL("https://datahub.io/core/s-and-p-500-companies-financials#r")
tab_f500_all <- get_f500() %>%
  # Function from the janitor package, makes nice column names
  clean_names() %>%
  # Arrange, so that the highest market cap appears first in the dataframe
  arrange(desc(market_cap)) 

write.xlsx(tab_f500_all, lst_paths$path_f500, TRUE)

# Select only the firms with the highest market cap
tab_f500_t10 <- slice(tab_f500_all, 1:.n_companies)

# Quick look at the companies we use
select(tab_f500_t10, symbol, name, sector)[1:10, ] %>%
  show_table()
```

## Get Company Index-Links

Before we attempt to download data in bulk, we let us first explore the SEC's EDGAR website:
```{r}
# browseURL(tab_f500_all$sec_filings[1])
```


The first step is to retrieve the index-links for the fortune 500 companies we selected in the first step. Downloading data from the web is always tricky. We can run into request limits, client or server side issues. So thinking about how to set up a download is crucial in order to make the analysis

We use a custom function: **map_company_filings()** to retrieve the Index Links from EDGAR.

### Function: map_company_filings()

```{r echo=FALSE}
# View(map_company_filings)
print(map_company_filings)
```

### Download Index-Links

Here we use a really simple caching procedure so that we don't have to re-run the whole expression if we already extracted index-links.

```{r}
if (!file.exists(lst_paths$path_filing)) {
  .prc <- tibble(symbol = character(), .rows = 0)
} else {
  .prc <- read_rds(lst_paths$path_filing)
}

tab_f500_use <- filter(tab_f500_t10, !symbol %in% .prc$symbol)

if (nrow(tab_f500_use) > 0) {
  lst_filings <- map_company_filings(
    .tickers = tab_f500_use$symbol, .type = "10-K", .count = 100, 
    .sleep = .2, .progress = FALSE
    )
  tab_filings <- bind_rows(.prc, bind_rows(lst_filings$result, .id = "symbol"))
  write_rds(tab_filings, lst_paths$path_filing)
} else {
  tab_filings <- .prc
}

tab_filings <- tab_filings %>%
  mutate(id = stri_replace_all_fixed(
    basename(href), paste0(".", file_ext(href)), "")
    ) %>% distinct(id, .keep_all = TRUE)

```

In total we got 8 errors.

```{r}
tab_f500_t10 %>%
  filter(!symbol %in% read_rds(lst_paths$path_filing)$symbol) %>%
  show_table(.n = 5)
# browseURL("https://www.sec.gov/cgi-bin/browse-edgar?company=BRK.B&match=&filenum=&State=&Country=&SIC=&myowner=exclude&action=getcompany")
```

Let's quickly look at the result. (There are several ways to do this. For small Dataframes we can simply use the RStudio build-in viewer)

```{r}
show_table(tab_filings, .n = 5)
```

## Get Company Details

After we got the index links from EDGAR, we proceed by scraping filing details. Again, we use a custom function to do this **map_filing_details()** and use a simple caching algorithm.

### Function: map_filing_details()

```{r echo=FALSE}
print(map_filing_details)
```

### Download Details

```{r}
if (!file.exists(lst_paths$path_detail)) {
  .prc <- tibble(id = character(), .rows = 0)
} else {
  .prc <- read_rds(lst_paths$path_detail)
}
tab_filings_use <- filter(tab_filings, !id %in% .prc$id)

if (nrow(tab_filings_use) > 0) {
  lst_details <- map_filing_details(
    .id = tab_filings_use$id, .hrefs = tab_filings_use$href, .sleep = 1
    )
  lst_details <- transpose(lst_details$result)
  lst_details <- map(lst_details, ~ bind_rows(.x, .id = "id"))
  tab_details <- reduce(lst_details, left_join, by = "id")
  
  tab_details <- bind_rows(.prc, .tab_details)
  write_rds(.tab_details, lst_paths$path_detail)
} else {
  tab_details <- .prc
}
rm(tab_filings_use)

```

From the **`r comma(nrow(tab_filings))`** index links we retrieved in the last step, we got **`r comma(nrow(tab_details))`** different document links.

It's important to notice, that such data retrieval tasks often result in very large datasets.

```{r}
# View(tab_details)
tab_details %>%
  mutate(year = year(period_date)) %>%
  rename(type = type.x) %>%
  group_by(type, year) %>%
  count() %>%
  pivot_wider(names_from = year, values_from = n, names_sort = TRUE, values_fill = 0) %>%
  show_table()
```

## Select Documents for Download

In order to reduce the amount of documents we download, we pre-select specific documents.

```{r}
tab_download <- tab_details %>%
  distinct() %>%
  left_join(select(tab_filings, symbol, id), by = "id") %>%
  mutate(
    file_ext = tools::file_ext(href),
    year = year(period_date),
    size = size / 1e6,
    across(where(is.character), ~ stri_replace_all_regex(., "[[:blank:]]+", " ")),
    across(c(type.y, description.y), ~ if_else(. %in% c("", " "), NA_character_, .))
    ) %>%
  select(id, document, state = company_incorporation_state, 
         sic = sic_code, year, symbol, company_name, company_cik, 
         type1 = type.x, type2 = type.y, desc = description.y, 
         period_date, href, size, file_ext) %>%
  filter(between(year, 2005, 2020))
```

```{r}
tab_download %>%
  rename(type = type1) %>%
  group_by(type, year) %>%
  count() %>%
  pivot_wider(names_from = year, values_from = n, names_sort = TRUE, values_fill = 0) %>%
  show_table()
```

## Overview: Number and Size of Documents

As you can see in the plot and the prints below, getting data from the web can result in really big data sets. (Keep in mind that we only downloaded data for 100 companies). The **download_edgar_files()** function will alleviate this problem by zipping data before writing to disk.

```{r}
.tmp <- tab_download %>%
  group_by(year) %>%
  summarise(size = sum(size), n = n(), .groups = "drop")


.geom_size <- .tmp %>%
  ggplot(aes(x = year, y = size)) +
  geom_line(color = "blue") + 
  geom_point() + 
  labs(x = NULL, y = NULL) + 
  scale_y_continuous(labels = scales::comma) + 
  theme_bw() + 
  ggtitle("Size per Year (in MB)")

.geom_n <- .tmp %>%
  ggplot(aes(x = year, y = n)) +
  geom_col(fill = "blue") + 
  labs(x = NULL, y = NULL) + 
  scale_y_continuous(labels = scales::comma) + 
  theme_bw() + 
  ggtitle("Number of Documents per year")

.geom_size / .geom_n
  
```

```{r}
tab_download_txt <- tab_download %>%
  filter(file_ext == "txt", desc == "Complete submission text file") %>%
  arrange(symbol, desc(year), desc(period_date)) %>%
  distinct(symbol, year, .keep_all = TRUE) %>%
  distinct(document, .keep_all = TRUE)

cat(paste0(
  "Docs: ", comma(nrow(tab_download_txt)), "\n",
  "Size: ", comma(sum(tab_download_txt$size)), " MB")
  )
```

```{r}
tab_download_htm <- tab_download %>%
  filter(startsWith(file_ext, "htm"), grepl("10-K", desc)) %>%
  arrange(symbol, desc(year), desc(period_date)) %>%
  distinct(symbol, year, .keep_all = TRUE) %>%
  distinct(document, .keep_all = TRUE)

cat(paste0(
  "Docs: ", comma(nrow(tab_download_htm)), "\n",
  "Size: ", comma(sum(tab_download_htm$size)), " MB")
  )
```

```{r}
tab_download_xxx <- tab_download %>%
  filter(startsWith(file_ext, "x")) %>%
  arrange(symbol, desc(year), desc(period_date)) %>%
  distinct(document, .keep_all = TRUE)

cat(paste0(
  "Docs: ", comma(nrow(tab_download_xxx)), "\n",
  "Size: ", comma(sum(tab_download_xxx$size)), " MB")
  )
```

## Download Documents

Again we use a custom function **download_edgar_files()** to download the documents.

### Function: download_edgar_files()

```{r echo=FALSE}
print(download_edgar_files)
```

### Download Files

```{r}
.dir_docs <- "2_output/01_get_edgar_data/documents/"
tab_xxx <- download_edgar_files(tab_download_xxx, .dir_docs, 10, 2)
tab_htm <- download_edgar_files(tab_download_htm, .dir_docs, 10, 2)
tab_txt <- download_edgar_files(tab_download_txt, .dir_docs, 10, 2)

tab_zip_files <- list_files_tab(.dir_docs, info = TRUE) %>%
  select(doc_id, file_ext, path, size) %>%
  mutate(size = size / 1e6)
```

Below you can see that we were able to reduce the dataset from more than 40 GB to just 3.7 GB by simply zipping the output.

```{r}
cat(paste0(
  "Docs: ", comma(nrow(tab_zip_files)), "\n",
  "Size: ", comma(sum(tab_zip_files$size)), " MB")
  )
```

## Save Files

```{r}
write_rds(tab_download_htm, "2_output/01_get_edgar_data/htm_download.rds")
write_rds(tab_download_txt, "2_output/01_get_edgar_data/txt_download.rds")
write_rds(tab_download_xxx, "2_output/01_get_edgar_data/xxx_download.rds")
```

# Own Function Calls

```{r}
lsf.str()
```

# Session Info

```{r}
sessioninfo::session_info()
```
