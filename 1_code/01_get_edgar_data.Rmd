---
title: "Getting Data from SEC's EDGAR"
author: "Matthias Uckert"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---

# Description

This script

```{r global_options}
knitr::opts_chunk$set(fig.path='Figs/')
```

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_knit$set(root.dir = normalizePath(here::here())) 
knitr::opts_chunk$set(root.dir = normalizePath(here::here())) 
```

# Script Setup

```{r message=FALSE, warning=FALSE}
library(tidyverse); library(edgarWebR); library(lubridate); library(here)
library(furrr); library(stringi); library(textstem); library(tidytext)
library(janitor); library(tools); library(patchwork); library(scales)
library(summarytools); library(kableExtra)

source("1_code/00_functions/f-all.R")
source("1_code/00_functions/f-get_edgar_data.R")
```

# Code

## Paths

```{r paths}
lst_paths <- list(
  dir_main    = "2_output/01_get_edgar_data/",
  path_filing = "2_output/01_get_edgar_data/edgar_filings.rds",
  path_detail = "2_output/01_get_edgar_data/edgar_details.rds"
) %>% create_dirs()

```

## Get Fortune 500 Companies

Her we use an open source data repository 'datahub.io' to retrieve the [**fortune 500 companies**]{.ul} with some financial information. We won't use the complete data set, but filter for the top 100 companies (using less companies will let the code run faster) with the highest market capitalization. You can change the number of companies by changing the integer vector: **.n_companies** in the chunk below.

Throughout this script we will use custom functions to download and transform data. All the script specific functions are stored in this folder: **1_code/00_functions/f-get_edgar_data.R**

**Note:** Look at the function [**get_f500()**]{.ul}

```{r choose-n-companies}
.n_companies <- 100
```

```{r}
# View(get_f500)
```

```{r get-fortune500-firms, message=FALSE, warning=FALSE}
  # browseURL("https://datahub.io/core/s-and-p-500-companies-financials#r")
tab_f500_all <- get_f500() %>%
  # Function from the janitor package, makes nice column names
  clean_names() %>%
  # Arrange, so that the highest market cap appears first in the dataframe
  arrange(desc(market_cap)) 

# Select only the firms with the highest market cap
tab_f500_t10 <- slice(tab_f500_all, 1:.n_companies)

# Quick look at the companies we use
select(tab_f500_t10, symbol, name, sector)

```

## Get Company Index-Links

Retrieve EDGAR Index-Links for Fortune 500 company set. The first step is to retrieve the index-links for the fortune 500 companies we selected in the first step. Downloading data from the web is always tricky. We can run into request limits, client or server side issues. So thinking about how to set up a download is crucial in order to make the analysis

We use a custom function: **map_company_filings()** to retrieve the Index Links from EDGAR.

```{r}
# View(map_company_filings)
```

Here we use a really simple caching procedure so that we don't have to re-run the whole expression if we already extracted index-links.

```{r}
if (!file.exists(lst_paths$path_filing)) {
  .prc <- tibble(symbol = character(), .rows = 0)
} else {
  .prc <- read_rds(lst_paths$path_filing)
}

tab_f500_use <- filter(tab_f500_t10, !symbol %in% .prc$symbol)

if (nrow(tab_f500_use) > 0) {
  lst_filings <- map_company_filings(
    .tickers = tab_f500_use$symbol, .type = "10-K", .count = 100, .sleep = .2
    )
  tab_filings <- bind_rows(.prc, bind_rows(lst_filings$result, .id = "symbol"))
  write_rds(tab_filings, lst_paths$path_filing)
} else {
  tab_filings <- .prc
}

tab_filings <- tab_filings %>%
  mutate(id = stri_replace_all_fixed(
    basename(href), paste0(".", file_ext(href)), "")
    ) %>% distinct(id, .keep_all = TRUE)

```

In total we got 8 errors.

```{r}
filter(tab_f500_t10, !symbol %in% read_rds(lst_paths$path_filing)$symbol)
browseURL("https://www.sec.gov/cgi-bin/browse-edgar?company=BRK.B&match=&filenum=&State=&Country=&SIC=&myowner=exclude&action=getcompany")
```

Let's quickly look at the result. (There are several ways to do this. For small Dataframes we can simply use the RStudio build-in viewer)

```{r}
glimpse(tab_filings, 100)
# View(tab_filings)
```

## Get Company Details

After we got the index links from EDGAR, we proceed by scraping filing details. Again, we use a custom function to do this **map_filing_details()** and use a simple caching algorithm.

```{r}
if (!file.exists(lst_paths$path_detail)) {
  .prc <- tibble(id = character(), .rows = 0)
} else {
  .prc <- read_rds(lst_paths$path_detail)
}
tab_filings_use <- filter(tab_filings, !id %in% .prc$id)

if (nrow(tab_filings_use) > 0) {
  lst_details <- map_filing_details(
    .id = tab_filings_use$id, .hrefs = tab_filings_use$href, .sleep = 1
    )
  lst_details <- transpose(lst_details$result)
  lst_details <- map(lst_details, ~ bind_rows(.x, .id = "id"))
  tab_details <- reduce(lst_details, left_join, by = "id")
  
  tab_details <- bind_rows(.prc, .tab_details)
  write_rds(.tab_details, lst_paths$path_detail)
} else {
  tab_details <- .prc
}
rm(tab_filings_use)

```


From the 2,364 index links we retrieved in the last step, we got 89,302 different document links.
It's important to notice, that such data retrieval tasks often result in very large datasets.
```{r}
glimpse(tab_details, 100)
# View(tab_details)
tab_details %>%
  mutate(year = year(period_date)) %>%
  group_by(type.x, year) %>%
  count() %>%
  pivot_wider(
    names_from = type.x, 
    values_from = n, 
    names_sort = TRUE,
    values_fill = 0
    )
```


## Select Documents for Download

In order to reduce the amount of documents we download, we pre-select specific documents.

```{r}
tab_download <- tab_details %>%
  distinct() %>%
  left_join(select(tab_filings, symbol, id), by = "id") %>%
  mutate(
    file_ext = tools::file_ext(href),
    year = year(period_date),
    size = size / 1e6,
    across(where(is.character), ~ stri_replace_all_regex(., "[[:blank:]]+", " ")),
    across(c(type.y, description.y), ~ if_else(. %in% c("", " "), NA_character_, .))
    ) %>%
  select(id, document, state = company_incorporation_state, 
         sic = sic_code, year, symbol, company_name, company_cik, 
         type1 = type.x, type2 = type.y, desc = description.y, 
         period_date, href, size, file_ext) %>%
  filter(between(year, 2005, 2020))
```


```{r}
glimpse(tab_download, 100)
tab_download %>%
  group_by(type1, year) %>%
  count() %>%
  pivot_wider(
    names_from = type1, 
    values_from = n, 
    names_sort = TRUE,
    values_fill = 0
  )
```


```{r}
.tmp <- tab_download %>%
  group_by(year) %>%
  summarise(size = sum(size), n = n(), .groups = "drop")


.geom_size <- .tmp %>%
  ggplot(aes(x = year, y = size)) +
  geom_line(color = "blue") + 
  geom_point() + 
  labs(x = NULL, y = NULL) + 
  scale_y_continuous(labels = scales::comma) + 
  theme_bw() + 
  ggtitle("Size per Year (in MB)")

.geom_n <- .tmp %>%
  ggplot(aes(x = year, y = n)) +
  geom_col(fill = "blue") + 
  labs(x = NULL, y = NULL) + 
  scale_y_continuous(labels = scales::comma) + 
  theme_bw() + 
  ggtitle("Number of Documents per year")

.geom_size / .geom_n
  
```

```{r}
tab_download_txt <- tab_download %>%
  filter(file_ext == "txt", desc == "Complete submission text file") %>%
  arrange(symbol, desc(year), desc(period_date)) %>%
  distinct(symbol, year, .keep_all = TRUE) %>%
  distinct(document, .keep_all = TRUE)

cat(paste0(
  "Docs: ", comma(nrow(tab_download_txt)), "\n",
  "Size: ", comma(sum(tab_download_txt$size)), " MB")
  )
```

```{r}
tab_download_htm <- tab_download %>%
  filter(startsWith(file_ext, "htm"), grepl("10-K", desc)) %>%
  arrange(symbol, desc(year), desc(period_date)) %>%
  distinct(symbol, year, .keep_all = TRUE) %>%
  distinct(document, .keep_all = TRUE)

cat(paste0(
  "Docs: ", comma(nrow(tab_download_htm)), "\n",
  "Size: ", comma(sum(tab_download_htm$size)), " MB")
  )
```

```{r}
tab_download_xxx <- tab_download %>%
  filter(startsWith(file_ext, "x")) %>%
  arrange(symbol, desc(year), desc(period_date)) %>%
  distinct(document, .keep_all = TRUE)

cat(paste0(
  "Docs: ", comma(nrow(tab_download_xxx)), "\n",
  "Size: ", comma(sum(tab_download_xxx$size)), " MB")
  )
```

## Download Documents

Again we use a custom function **download_edgar_files()** to download the documents.

```{r}
.dir_docs <- "2_output/01_get_edgar_data/documents/"
tab_xxx <- download_edgar_files(tab_download_xxx, .dir_docs, 10, 2)
tab_htm <- download_edgar_files(tab_download_htm, .dir_docs, 10, 2)
tab_txt <- download_edgar_files(tab_download_txt, .dir_docs, 10, 2)

tab_zip_files <- list_files_tab(.dir_docs, info = TRUE) %>%
  select(doc_id, file_ext, path, size) %>%
  mutate(size = size / 1e6)
```


```{r}
cat(paste0(
  "Docs: ", comma(nrow(tab_zip_files)), "\n",
  "Size: ", comma(sum(tab_zip_files$size)), " MB")
  )
```

## Save Files

```{r}
write_rds(tab_download_htm, "2_output/01_get_edgar_data/htm_download.rds")
write_rds(tab_download_txt, "2_output/01_get_edgar_data/txt_download.rds")
write_rds(tab_download_xxx, "2_output/01_get_edgar_data/xxx_download.rds")
```
